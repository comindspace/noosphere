{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!poetry update -q",
   "id": "c8d46148ab432be6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from configurations.cypher_chat_openai import CypherChatOpenAIConfiguration\n",
    "from configurations.neo4j import Neo4jConfiguration\n",
    "from configurations.configuration import Configuration\n",
    "from configurations.chat_openai import ChatOpenAIConfiguration\n",
    "\n",
    "configurations = Configuration.load('../configuration.yaml')\n",
    "neo4j = Neo4jConfiguration.grab(configurations)\n",
    "chat_openai = ChatOpenAIConfiguration.grab(configurations)\n",
    "cypher_chat_openai = CypherChatOpenAIConfiguration.grab(configurations)"
   ],
   "id": "9d795c1d9dac93b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "from frames_benchmark_record import FramesBenchmarkRecord\n",
    "from load_csv import load_csv\n",
    "\n",
    "tests = load_csv('test.tsv', FramesBenchmarkRecord, delimiter='\\t')\n",
    "with open('imported_links.txt', 'r') as f:\n",
    "    imported_links = [line.strip() for line in f.readlines()]\n",
    "tests_for_imported: list[FramesBenchmarkRecord] = list(filter(lambda t: all((wiki_link in imported_links) for wiki_link in t.wiki_links), tests))"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.chains.graph_qa.cypher import GraphCypherQAChain\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "graph = Neo4jGraph(**neo4j.dict())\n",
    "CYPHER_GENERATION_TEMPLATE = \"\"\"Task:Generate Cypher statement to query a graph database.\n",
    "    Schema:\n",
    "    {schema}\n",
    "    Note: Do not include any explanations or apologies in your responses.\n",
    "    Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\n",
    "    Do not include any text except the generated Cypher statement.\n",
    "    \n",
    "    The question is:\n",
    "    {question}\"\"\"\n",
    "\n",
    "CYPHER_GENERATION_PROMPT = PromptTemplate(input_variables=['schema', 'question'], validate_template=True, template=CYPHER_GENERATION_TEMPLATE)\n",
    "\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    ChatOpenAI(**cypher_chat_openai.dict()),\n",
    "    cypher_prompt=CYPHER_GENERATION_PROMPT,\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True,\n",
    ")\n",
    "\n",
    "response = chain.invoke({'query': tests_for_imported[0].prompt})\n",
    "response"
   ],
   "id": "8850cfe04054b1be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "prompt = \"\"\"===Task===\n",
    "\n",
    "I need your help in evaluating an answer provided by an LLM against a ground truth answer. Your task is to determine if the ground truth answer is present in the LLM’s response. Please analyze the provided data and make a decision.\n",
    "===Instructions===\n",
    "1. Carefully compare the \"Predicted Answer\" with the \"Ground Truth Answer\".\n",
    "2. Consider the substance of the answers – look for equivalent information or correct answers. Do not focus on exact wording unless the exact wording is crucial to the meaning.\n",
    "3. Your final decision should be based on whether the meaning and the vital facts of the \"Ground Truth Answer\" are present in the \"Predicted Answer:\"\n",
    "\n",
    "===Input Data===\n",
    "- Question: «{question}»\n",
    "- Predicted Answer: «{answer}»\n",
    "- Ground Truth Answer: «{ground_truth}»\n",
    "\n",
    "===Output Format===\n",
    "Provide your final evaluation in the following format:\n",
    "\"Explanation:\" (How you made the decision?)\n",
    "\"Decision:\" (\"TRUE\" or \"FALSE\" )\n",
    "\n",
    "Please proceed with the evaluation. \"\"\"\n",
    "\n",
    "chat = ChatOpenAI(**chat_openai.dict())\n",
    "message = chat.invoke([\n",
    "    SystemMessage(content=prompt),\n",
    "    HumanMessage(content=tests_for_imported[0].prompt)\n",
    "])\n",
    "print(message.content)"
   ],
   "id": "e4697c5f90d05a28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# for t in tests:\n",
    "#     if all((wiki_link in imported_links) for wiki_link in t.wiki_links):\n",
    "#         print(t.id)"
   ],
   "id": "c78bd2031a3a1514",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
